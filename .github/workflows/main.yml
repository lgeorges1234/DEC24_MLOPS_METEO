name: Deploy Weather App Pipeline

on:
  push:
    branches: [ main, master ]
  pull_request:
    branches: [ main, master ]
  workflow_dispatch:
    inputs:
      environment:
        description: 'Environment to deploy to'
        required: true
        default: 'production'
        type: choice
        options:
          - development
          - staging
          - production

env:
  AIRFLOW_IMAGE_NAME: apache/airflow:2.8.1
  AIRFLOW_UID: 50000
  AIRFLOW_GID: 50000
  _AIRFLOW_WWW_USER_USERNAME: airflow
  _AIRFLOW_WWW_USER_PASSWORD: ${{ secrets.AIRFLOW_PASSWORD }}
  DB_NAME: ${{ secrets.DB_NAME }}
  DB_USER: ${{ secrets.DB_USER }}
  DB_PASSWORD: ${{ secrets.DB_PASSWORD }}
  DB_SECRET_KEY: ${{ secrets.DB_SECRET_KEY }}
  SMTP_PASSWORD: ${{ secrets.SMTP_PASSWORD }}

jobs:
  deploy:
    runs-on: [self-hosted, weather-app]
    environment: ${{ github.event.inputs.environment || 'production' }}
    steps:
      - name: Checkout code
        uses: actions/checkout@v3

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.9'

      - name: Create deployment directory
        run: |
          mkdir -p ~/weather-app-deployment
          
      - name: Copy deployment files
        run: |
          rsync -av --exclude='.git' ./ ~/weather-app-deployment/
          
      - name: Pull Docker images
        run: |
          # Pull all required Docker images
          docker pull postgres:13
          docker pull redis:latest
          docker pull ghcr.io/mlflow/mlflow:v2.21.0rc0
          docker pull ${AIRFLOW_IMAGE_NAME}
          
      - name: Build API Docker image
        run: |
          cd ~/weather-app-deployment/app/api
          docker build -t weather-app:latest .
      
      - name: Configure environment
        run: |
          cd ~/weather-app-deployment
          
          # Create .env file from secrets
          cat > .env << EOF
          AIRFLOW_UID=${AIRFLOW_UID}
          AIRFLOW_GID=${AIRFLOW_GID}
          _AIRFLOW_WWW_USER_USERNAME=${_AIRFLOW_WWW_USER_USERNAME}
          _AIRFLOW_WWW_USER_PASSWORD=${_AIRFLOW_WWW_USER_PASSWORD}
          DB_NAME=${DB_NAME}
          DB_USER=${DB_USER}
          DB_PASSWORD=${DB_PASSWORD}
          DB_SECRET_KEY=${DB_SECRET_KEY}
          EOF
          
          # Update SMTP password in docker-compose
          # sed -i "s/AIRFLOW__SMTP__SMTP_PASSWORD: \"cfsrvkongsobheta\"/AIRFLOW__SMTP__SMTP_PASSWORD: \"${SMTP_PASSWORD}\"/" docker-compose.yml
      
      - name: Start services
        run: |
          cd ~/weather-app-deployment
          docker-compose down
          docker-compose up -d
      
      - name: Initialize data directories
        run: |
          cd ~/weather-app-deployment
          
          # Make sure directories have correct permissions
          sudo chown -R ${AIRFLOW_UID}:${AIRFLOW_GID} airflow
          sudo chown -R ${AIRFLOW_UID}:${AIRFLOW_GID} raw_data
          sudo chown -R ${AIRFLOW_UID}:${AIRFLOW_GID} api
      
      - name: Verify deployment
        run: |
          cd ~/weather-app-deployment
          
          # Wait for services to be fully up
          echo "Waiting for services to start up..."
          sleep 90
          
          # Check if Airflow is running
          if curl -s http://localhost:8080/health > /dev/null; then
            echo "Airflow is running"
          else
            echo "Airflow is not running"
            exit 1
          fi
          
          # Check if MLflow is running
          if curl -s http://localhost:5000/api/2.0/mlflow/experiments/list > /dev/null; then
            echo "MLflow is running"
          else
            echo "MLflow is not running"
            exit 1
          fi
          
          # Check if the API is running
          if curl -s http://localhost:8000/docs > /dev/null; then
            echo "Weather API is running"
          else
            echo "Weather API is not running"
            exit 1
          fi
          
          echo "Deployment successful!"

  trigger-initial-pipeline:
    needs: deploy
    runs-on: [self-hosted, weather-app]
    if: github.event_name == 'workflow_dispatch' && github.event.inputs.environment == 'production'
    steps:
      - name: Trigger initial data split and training
        run: |
          # Wait a bit more for Airflow to be fully initialized
          sleep 60
          
          # Trigger the data loading DAG if it exists
          curl -X POST http://localhost:8080/api/v1/dags/1_weather_data_loading_dag/dagRuns \
            -H "Content-Type: application/json" \
            -u "${_AIRFLOW_WWW_USER_USERNAME}:${_AIRFLOW_WWW_USER_PASSWORD}" \
            -d '{"conf": {}}'
          
          echo "Initial pipeline triggered"