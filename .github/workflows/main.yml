name: Deploy Weather App Pipeline

on:
  push:
    branches: [ main, master ]
  pull_request:
    branches: [ main, master ]
  workflow_dispatch:
    inputs:
      environment:
        description: 'Environment to deploy to'
        required: true
        default: 'production'
        type: choice
        options:
          - development
          - staging
          - production

env:
  AIRFLOW_IMAGE_NAME: apache/airflow:2.8.1
  AIRFLOW_UID: 50000
  AIRFLOW_GID: 50000
  _AIRFLOW_WWW_USER_USERNAME: airflow
  _AIRFLOW_WWW_USER_PASSWORD: ${{ secrets.AIRFLOW_PASSWORD }}
  DB_NAME: ${{ secrets.DB_NAME }}
  DB_USER: ${{ secrets.DB_USER }}
  DB_PASSWORD: ${{ secrets.DB_PASSWORD }}
  DB_SECRET_KEY: ${{ secrets.DB_SECRET_KEY }}
  SMTP_PASSWORD: ${{ secrets.SMTP_PASSWORD }}

jobs:
  deploy:
    runs-on: [self-hosted, weather-app]
    environment: ${{ github.event.inputs.environment || 'production' }}
    steps:
      - name: Checkout code
        uses: actions/checkout@v3

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.9'

      - name: Create deployment directory and copy files
        run: |
          # Create deployment directory
          mkdir -p ~/weather-app-deployment
          
          # Create all required subdirectories
          mkdir -p ~/weather-app-deployment/airflow/{dags,logs,plugins}
          mkdir -p ~/weather-app-deployment/raw_data/{initial_dataset,training_raw_data,prediction_raw_data}
          mkdir -p ~/weather-app-deployment/api/data/{prepared_data,metrics,models}
          
          # Copy files from repository
          cp docker-compose.yml ~/weather-app-deployment/
          
          # Debug - list current directory contents
          echo "Current working directory content:"
          ls -la
          
          # Copy API directory if it exists
          if [ -d "./api" ]; then
            cp -r ./api ~/weather-app-deployment/
            echo "API directory copied"
          else
            echo "WARNING: API directory not found in repository root"
            find . -name "api" -type d
          fi
          
          # Copy airflow directory if it exists
          if [ -d "./airflow" ]; then
            cp -r ./airflow ~/weather-app-deployment/
            echo "Airflow directory copied"
          else
            echo "WARNING: Airflow directory not found"
          fi
          
      - name: Pull Docker images
        run: |
          # Pull all required Docker images
          docker pull postgres:13
          docker pull redis:latest
          docker pull ghcr.io/mlflow/mlflow:v2.21.0rc0
          docker pull ${AIRFLOW_IMAGE_NAME}
          
      - name: Build API Docker image
        run: |
          # First check if the API directory exists
          if [ -d "~/weather-app-deployment/api" ]; then
            cd ~/weather-app-deployment/api
            docker build -t weather-app:latest .
          elif [ -d "./api" ]; then
            # Build directly from the checked out code
            cd ./api
            docker build -t weather-app:latest .
            # Then make sure it's copied to the deployment directory
            mkdir -p ~/weather-app-deployment/api
            cp -r . ~/weather-app-deployment/api/
          else
            echo "ERROR: API directory not found in either location"
            find . -type d -name "api" | xargs ls -la
            exit 1
          fi
      
      - name: Configure environment
        run: |
          # Create .env file from secrets
          cat > ~/weather-app-deployment/.env << EOF
          AIRFLOW_UID=${AIRFLOW_UID}
          AIRFLOW_GID=${AIRFLOW_GID}
          _AIRFLOW_WWW_USER_USERNAME=${_AIRFLOW_WWW_USER_USERNAME}
          _AIRFLOW_WWW_USER_PASSWORD=${_AIRFLOW_WWW_USER_PASSWORD}
          DB_NAME=${DB_NAME}
          DB_USER=${DB_USER}
          DB_PASSWORD=${DB_PASSWORD}
          DB_SECRET_KEY=${DB_SECRET_KEY}
          EOF
          
          # Check if docker-compose exists
          if [ -f "~/weather-app-deployment/docker-compose.yml" ]; then
            # Update SMTP password in docker-compose
            sed -i "s/AIRFLOW__SMTP__SMTP_PASSWORD: \"cfsrvkongsobheta\"/AIRFLOW__SMTP__SMTP_PASSWORD: \"${SMTP_PASSWORD}\"/" ~/weather-app-deployment/docker-compose.yml
          elif [ -f "./docker-compose.yml" ]; then
            # Copy and update docker-compose.yml
            cp ./docker-compose.yml ~/weather-app-deployment/
            sed -i "s/AIRFLOW__SMTP__SMTP_PASSWORD: \"cfsrvkongsobheta\"/AIRFLOW__SMTP__SMTP_PASSWORD: \"${SMTP_PASSWORD}\"/" ~/weather-app-deployment/docker-compose.yml
          else
            echo "ERROR: docker-compose.yml not found"
            find . -name "docker-compose.yml"
            exit 1
          fi
          
          # Debug - list deployment directory
          echo "Deployment directory content:"
          ls -la ~/weather-app-deployment/
      
      - name: Start services
        run: |
          cd ~/weather-app-deployment
          
          # Check if docker-compose.yml exists
          if [ ! -f "docker-compose.yml" ]; then
            echo "ERROR: docker-compose.yml not found in deployment directory"
            ls -la
            exit 1
          fi
          
          # Stop any running containers
          docker-compose down || true
          
          # Start services in detached mode
          docker-compose up -d
      
      - name: Initialize data directories
        run: |
          cd ~/weather-app-deployment
          
          # Make sure directories have correct permissions
          sudo chown -R ${AIRFLOW_UID}:${AIRFLOW_GID} airflow
          sudo chown -R ${AIRFLOW_UID}:${AIRFLOW_GID} raw_data
          sudo chown -R ${AIRFLOW_UID}:${AIRFLOW_GID} api
      
      - name: Verify deployment
        run: |
          cd ~/weather-app-deployment
          
          # Wait for services to be fully up
          echo "Waiting for services to start up..."
          sleep 30
          
          # Check if Airflow is running
          if curl -s http://localhost:8080/health > /dev/null; then
            echo "Airflow is running"
          else
            echo "Airflow is not running"
            exit 1
          fi
          
          # Check if MLflow is running
          if curl -s http://localhost:5000/api/2.0/mlflow/experiments/list > /dev/null; then
            echo "MLflow is running"
          else
            echo "MLflow is not running"
            exit 1
          fi
          
          # Check if the API is running
          if curl -s http://localhost:8000/docs > /dev/null; then
            echo "Weather API is running"
          else
            echo "Weather API is not running"
            exit 1
          fi
          
          echo "Deployment successful!"

  trigger-initial-pipeline:
    needs: deploy
    runs-on: [self-hosted, weather-app]
    if: github.event_name == 'workflow_dispatch' && github.event.inputs.environment == 'production'
    steps:
      - name: Trigger initial data split and training
        run: |
          # Wait a bit more for Airflow to be fully initialized
          sleep 60
          
          # Trigger the data loading DAG if it exists
          curl -X POST http://localhost:8080/api/v1/dags/1_weather_data_loading_dag/dagRuns \
            -H "Content-Type: application/json" \
            -u "${_AIRFLOW_WWW_USER_USERNAME}:${_AIRFLOW_WWW_USER_PASSWORD}" \
            -d '{"conf": {}}'
          
          echo "Initial pipeline triggered"