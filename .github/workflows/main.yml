name: Deploy Weather App Pipeline

on:
  push:
    branches: [ main, master ]
  pull_request:
    branches: [ main, master ]
  workflow_dispatch:
    inputs:
      environment:
        description: 'Environment to deploy to'
        required: true
        default: 'production'
        type: choice
        options:
          - development
          - staging
          - production

env:
  AIRFLOW_IMAGE_NAME: apache/airflow:2.8.1
  AIRFLOW_UID: 50000
  AIRFLOW_GID: 50000
  _AIRFLOW_WWW_USER_USERNAME: airflow
  _AIRFLOW_WWW_USER_PASSWORD: ${{ secrets.AIRFLOW_PASSWORD }}
  DB_NAME: ${{ secrets.DB_NAME }}
  DB_USER: ${{ secrets.DB_USER }}
  DB_PASSWORD: ${{ secrets.DB_PASSWORD }}
  DB_SECRET_KEY: ${{ secrets.DB_SECRET_KEY }}
  SMTP_PASSWORD: ${{ secrets.SMTP_PASSWORD }}

jobs:
  test:
    name: Run Unit Tests
    runs-on: [self-hosted, weather-app]
    steps:
      - name: Clean up Docker resources
        run: |
          # Display initial disk space
          echo "Initial disk space:"
          df -h
          
          # Stop and remove all containers
          echo "Stopping and removing containers..."
          docker stop $(docker ps -a -q) || true
          docker rm $(docker ps -a -q) || true
          
          # Remove dangling images (untagged images)
          echo "Removing dangling images..."
          docker image prune -f
          
          # Remove unused volumes
          echo "Removing unused volumes..."
          docker volume prune -f
          
          # Remove unused networks
          echo "Removing unused networks..."
          docker network prune -f
          
          # Full system prune for everything else
          echo "Performing system prune..."
          docker system prune -af --volumes
          
          # Display available space after cleanup
          echo "Disk space after cleanup:"
          df -h

      - name: Build test Docker image
        run: |
          docker build -t weather-app-tests -f app/api/Dockerfile.test .
          
      - name: Run unit tests
        run: |
          echo "Running unit tests..."
          # Run tests and capture exit code
          docker run --rm weather-app-tests
          TEST_EXIT_CODE=$?
          
          if [ $TEST_EXIT_CODE -ne 0 ]; then
            echo "Tests failed. Exiting with error code $TEST_EXIT_CODE"
            exit $TEST_EXIT_CODE
          else
            echo "All tests passed successfully"
          fi

  deploy:
    name: Deploy Weather App
    needs: test
    runs-on: [self-hosted, weather-app]
    environment: ${{ github.event.inputs.environment || 'production' }}
    steps:
      - name: Checkout code
        uses: actions/checkout@v3

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.9'

      - name: Create deployment directory
        run: |
          # Create new empty directory with proper permissions
          mkdir -p ~/weather-app-deployment
          sudo chown ubuntu:ubuntu ~/weather-app-deployment
          sudo chmod 775 ~/weather-app-deployment

          
      - name: Copy deployment files
        run: |
          # Copy only the app directory with sudo to avoid permission issues
          sudo cp -r ./app/* ~/weather-app-deployment/
          
          # Set proper ownership for the entire deployment directory
          sudo chown -R ubuntu:ubuntu ~/weather-app-deployment
          
          # Ensure directories have execute permissions
          sudo find ~/weather-app-deployment -type d -exec chmod 755 {} \;
          
          # Ensure files have read/write permissions
          sudo find ~/weather-app-deployment -type f -exec chmod 644 {} \;
          
          # Debug directory structure
          echo "Contents of deployment directory:"
          ls -la ~/weather-app-deployment/
          
      - name: Pull Docker images
        run: |
          # Pull all required Docker images
          docker pull postgres:13
          docker pull redis:latest
          docker pull ghcr.io/mlflow/mlflow:v2.21.0rc0
          docker pull ${AIRFLOW_IMAGE_NAME}

      - name: Configure environment
        run: |
          cd ~/weather-app-deployment
          
          # Create .env file from secrets
          cat > .env << EOF
          AIRFLOW_UID=${AIRFLOW_UID}
          AIRFLOW_GID=${AIRFLOW_GID}
          _AIRFLOW_WWW_USER_USERNAME=${_AIRFLOW_WWW_USER_USERNAME}
          _AIRFLOW_WWW_USER_PASSWORD=${_AIRFLOW_WWW_USER_PASSWORD}
          DB_NAME=${DB_NAME}
          DB_USER=${DB_USER}
          DB_PASSWORD=${DB_PASSWORD}
          DB_SECRET_KEY=${DB_SECRET_KEY}
          AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION=False
          EOF
          
          # Update SMTP password in docker-compose
          # sed -i "s/AIRFLOW__SMTP__SMTP_PASSWORD: \"cfsrvkongsobheta\"/AIRFLOW__SMTP__SMTP_PASSWORD: \"${SMTP_PASSWORD}\"/" docker-compose.yaml
      
      - name: Start services
        run: |
          cd ~/weather-app-deployment
          docker-compose down
          docker-compose up --build -d

      - name: Initialize data directories
        run: |
          cd ~/weather-app-deployment
          
          # Make sure directories have correct permissions
          sudo chown -R ${AIRFLOW_UID}:${AIRFLOW_GID} airflow
          sudo chown -R ${AIRFLOW_UID}:${AIRFLOW_GID} raw_data
          sudo chown -R ${AIRFLOW_UID}:${AIRFLOW_GID} api
      
      - name: Verify deployment
        run: |
          cd ~/weather-app-deployment
          
          # Define function to check service readiness
          check_service_ready() {
            local service_name=$1
            local url=$2
            local max_attempts=$3
            local wait_seconds=$4
            
            echo "Checking if $service_name is ready..."
            
            for ((i=1; i<=$max_attempts; i++)); do
              if curl -s "$url" > /dev/null; then
                echo "$service_name is ready!"
                return 0
              fi
              
              echo "Attempt $i/$max_attempts: $service_name not ready yet, waiting ${wait_seconds}s..."
              sleep $wait_seconds
            done
            
            echo "$service_name failed to start after $(($max_attempts * $wait_seconds)) seconds"
            return 1
          }
          
          # Maximum 30 attempts with 10 seconds wait = up to 5 minutes per service
          check_service_ready "Airflow" "http://localhost:8080/health" 30 10 || exit 1
          check_service_ready "MLflow" "http://localhost:5000/api/2.0/mlflow/experiments/list" 30 10 || exit 1
          check_service_ready "Weather API" "http://localhost:8000/docs" 30 10 || exit 1
          check_service_ready "Streamlit" "http://localhost:8501" 30 10 || exit 1

          echo "All services are up and running!"

  trigger-initial-pipeline:
    needs: deploy
    runs-on: [self-hosted, weather-app]
    steps:
      - name: Unpause DAGs and trigger initial workflow
        run: |
          # Wait for Airflow to fully initialize
          echo "Waiting 30 seconds for Airflow to fully initialize..."
          sleep 30
          
          # Specific DAGs we need to unpause and trigger
          DAGS=("1_weather_initial_split_dag" "2_weather_training_dag" "3_weather_prediction_dag")
          
          # Unpause each DAG using the official Airflow REST API
          for dag_id in "${DAGS[@]}"; do
            echo "Unpausing DAG: $dag_id"
            curl -X PATCH "http://localhost:8080/api/v1/dags/$dag_id" \
              -H "Content-Type: application/json" \
              -u "${_AIRFLOW_WWW_USER_USERNAME}:${_AIRFLOW_WWW_USER_PASSWORD}" \
              -d '{"is_paused": false}'
            
            # Small delay between API calls
            sleep 2
          done
          
          # Verify DAG status
          echo "Verifying DAG status:"
          for dag_id in "${DAGS[@]}"; do
            echo "Checking status of DAG: $dag_id"
            curl -s "http://localhost:8080/api/v1/dags/$dag_id" \
              -H "Content-Type: application/json" \
              -u "${_AIRFLOW_WWW_USER_USERNAME}:${_AIRFLOW_WWW_USER_PASSWORD}" | \
              python -c "import json, sys; data=json.load(sys.stdin); print(f\"DAG: {data['dag_id']} - Paused: {data['is_paused']}\")"
          done
          
          # Trigger the initial split DAG
          echo "Triggering the initial split DAG..."
          curl -X POST "http://localhost:8080/api/v1/dags/1_weather_initial_split_dag/dagRuns" \
            -H "Content-Type: application/json" \
            -u "${_AIRFLOW_WWW_USER_USERNAME}:${_AIRFLOW_WWW_USER_PASSWORD}" \
            -d '{"conf": {}}'
          
          echo "Initial pipeline successfully unpaused and triggered"