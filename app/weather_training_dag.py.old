from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.utils.task_group import TaskGroup
from airflow.sensors.filesystem import FileSensor 

import os
from datetime import datetime, timedelta
import requests
from pathlib import Path

# Set the api URL
API_URL = os.getenv('API_API_URL', 'http://app:8000')


# Use environment variables if set, otherwise fall back to relative paths
RAW_DATA_PATH = Path(os.getenv('RAW_DATA_PATH', '/raw_data/weatherAUS.csv'))
CLEAN_DATA_PATH = Path(os.getenv('PREPARED_DATA_PATH', '/prepared_data'))
METRICS_DATA_PATH = Path(os.getenv('METRICS_DATA_PATH', '/metrics'))
MODEL_PATH = Path(os.getenv('MODEL_PATH', '/models'))

def collect_weather_data(**context):
    """
    Collect weather data from raw data folder
    """
    try:
        response = requests.get(f"{API_URL}/extract", timeout=300)
        response.raise_for_status()
        result = response.json()
        print(f"Data extraction completed. Response: {result}")
        return result

    except requests.RequestException as e:
        print(f"Error making request to extract endpoint: {str(e)}")
        raise
    except Exception as e:
        print(f"Error in collect_weather_data: {str(e)}")
        raise

def train_model(**context):
    """
    Train model using extracted and preprocessed data
    """
    try:
        response = requests.get(f"{API_URL}/training", timeout=300)
        response.raise_for_status()
        result = response.json()
        print(f"Model training completed. Response: {result}")
        return result

    except requests.RequestException as e:
        print(f"Error making request to training endpoint: {str(e)}")
        raise
    except Exception as e:
        print(f"Error in train_model: {str(e)}")
        raise

def evaluate_model(**context):
    """
    Evaluate model that has been previously trained
    """
    try:
        response = requests.get(f"{API_URL}/evaluate", timeout=300)
        response.raise_for_status()
        result = response.json()
        print(f"Model evaluation completed. Response: {result}")
        return result

    except requests.RequestException as e:
        print(f"Error making request to training endpoint: {str(e)}")
        raise
    except Exception as e:
        print(f"Error in train_model: {str(e)}")
        raise

# DAG definition
default_args = {
    'owner': 'airflow',
    'depends_on_past': False,
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 1,
    'retry_delay': timedelta(minutes=1)
}

with DAG(
    'weather_training_pipeline',
    default_args=default_args,
    description='Weather data collection and model training pipeline',
    schedule_interval='0 0 * * MON',
    start_date=datetime(2025, 2, 19),
    catchup=False,
    tags=['weather', 'data_collection', 'data_preprocessing', 'model_training', 'model_evaluation']
) as dag:
    
    # Check for raw data file before extraction
    check_raw_file = FileSensor(
        task_id='check_raw_data_file',
        filepath=str(RAW_DATA_PATH),
        fs_conn_id='fs_default',
        poke_interval=30,
        timeout=600,
        mode='poke',
        dag=dag
    )

    with TaskGroup(group_id='data_collection') as data_collection:
        collect_weather = PythonOperator(
            task_id='collect_wether_data',
            python_callable=collect_weather_data,
            provide_context=True,
            dag=dag
        )

    # Check for cleaned data file before training
    check_clean_file = FileSensor(
        task_id='check_clean_data_file',
        filepath=str(CLEAN_DATA_PATH / "meteo.csv"),
        fs_conn_id='fs_default',
        poke_interval=30,
        timeout=600,
        mode='poke',
        dag=dag
    )

    with TaskGroup(group_id='model_training') as model_training:
        model_training = PythonOperator(
            task_id='train_model',
            python_callable=train_model,
            provide_context=True,
            dag=dag
        )

    # Check for training files before evaluation
    check_train_files = FileSensor(
        task_id='check_train_files',
        filepath=str(MODEL_PATH / "rfc.joblib"),
        fs_conn_id='fs_default',
        poke_interval=30,
        timeout=600,
        mode='poke',
        dag=dag
    )

    with TaskGroup(group_id='model_evaluation') as model_evaluation:
        model_evaluation = PythonOperator(
            task_id='evaluate_model',
            python_callable=evaluate_model,
            provide_context=True,
            dag=dag
        )

    # Check for training files before evaluation
    check_metrics_files = FileSensor(
        task_id='check_metrics_file',
        filepath=str(METRICS_DATA_PATH / "metrics.json"),
        fs_conn_id='fs_default',
        poke_interval=30,
        timeout=600,
        mode='poke',
        dag=dag
    )


# Set up task dependencies
    check_raw_file >> data_collection >> check_clean_file >> model_training >> check_train_files >> model_evaluation >> check_metrics_files

